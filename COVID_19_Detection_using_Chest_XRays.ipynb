{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "COVID-19 Detection using Chest XRays.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MXyspuHdcK3"
      },
      "source": [
        "#**COVID-19 Detection using Chest X-Rays**\n",
        "\n",
        "The pandemic, originated by Novel Coronavirus-2019 (COVID-19), continuing its devastating effect on the health, well-being, and economy of the global population. The early detection and diagnosis of COVID-19 and the accurate separation of COVID infected cases at the lowest cost in the early stage is the main challenge in the current scenario. A critical step to restrain this pandemic is the early detection of COVID-19 in the human body, to constraint the exposure and control the spread of the virus. Chest X-Rays are one of the non-invasive tools to detect this disease as the manual PCR diagnosis process is quite tedious and time-consuming. Concerning the novelty of the disease, diagnostic methods based on radiological images suffer from shortcomings despite their many applications in diagnostic centers. Accordingly, medical and computer researchers tend to use machine-learning models to analyze radiology images.\n",
        "<br><br>\n",
        "![COVID-19](https://eyewire.news/wp-content/uploads/sites/2/2020/03/banner.png)\n",
        "<br><br>\n",
        "\n",
        "In this project, we have attempted to develop an automated COVID-19 classifer, utilizing available COVID and non-COVID Chest X-Ray datasets. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOX0quPEgoh-"
      },
      "source": [
        "###**The following are the steps involved in this project:**\n",
        "- Importing the necessary packages\n",
        "- Data Collection and Preprocessing\n",
        "- Building a CNN based model in Keras\n",
        "- Compiling the Model\n",
        "- Processing the Training and Testing Images\n",
        "- Training the Model\n",
        "- Evaluating the Model\n",
        "- Saving the Model\n",
        "- Creating a Classification Method\n",
        "\n",
        "\n",
        "###**Let's start implementing the above steps one by one:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0Asw7clh62p"
      },
      "source": [
        "###**Importing the necessary packages:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgUBvi2I9GCy"
      },
      "source": [
        "# Import packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras.preprocessing import image\n",
        "from keras.metrics import accuracy, binary_crossentropy\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk7JGsbuiREJ"
      },
      "source": [
        "###**Data Collection and Preprocessing:**\n",
        "####**Data Collection:**\n",
        "For building this project we have to mainly rely on two types of Chest X-Rays. They are:\n",
        "1. **COVID** infected patients Chest X-Rays,\n",
        "2. **Non-COVID** patients Chest X-Rays like Pneumonia, Tuberculosis etc.\n",
        "\n",
        "For **COVID** X-Rays we have downloaded the Chest X-Rays from [GitHub.](https://github.com/ieee8023/covid-chestxray-dataset) (Mix of COVID Positive and other diseases)\n",
        "\n",
        "And for **Non-COVID** X-Rays we have downloaded the data from [Kaggle.](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia) (Chest X-Rays of Pneunomia patients)\n",
        "\n",
        "####**Data Preprocessing:**\n",
        "There are a total of 930 Chest X-Rays available at the time of doing this project, out of which 196 are labelled as **COVID Positive.** So we have extracted those images from the complete dataset using the metadata. \n",
        "\n",
        "And from the Pneunomia Chest X-Rays Dataset we have selected the equal number of **Normal Chest X-Rays** (196 Normal Chest X-Rays) and labelled them as **COVID Negative or Non COVID.** \n",
        "\n",
        "####**Data Split:**\n",
        "Out of the total available 392 X-Rays (196-COVID, 196-Non COVID), 25% (49 X-Rays) are separated for Validation and the remaining 75% (147 X-Rays) are used for Training the model.\n",
        "\n",
        "Seperated them in Train and Validation Datasets and organised in folders as follows:\n",
        "- Dataset\n",
        "  - Train\n",
        "    - COVID\n",
        "    - Normal\n",
        "  - Validation\n",
        "    - COVID\n",
        "    - Normal\n",
        "\n",
        "    and uploaded them to Dropbox for future use. The link of the seggregated data set is given [here](https://www.dropbox.com/s/tlsdn617iymz3bf/CovidDataset.zip) in `.zip` format.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2o2yejS8PZ0"
      },
      "source": [
        "# Downloading the dataset zip file from Dropbox\n",
        "if (not os.path.exists(\"CovidDataset.zip\")):\n",
        "  # Download the dataset file from dropbox\n",
        "  !wget https://www.dropbox.com/s/tlsdn617iymz3bf/CovidDataset.zip\n",
        "if (not os.path.exists(\"Dataset/\")):\n",
        "  # Unzip the filed\n",
        "  !unzip CovidDataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBPPkrRb87hD"
      },
      "source": [
        "# Declaring the path variables\n",
        "TRAIN_PATH = \"DataSet/Train\"\n",
        "VAL_PATH = \"DataSet/Val\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEkvN6v4ohW9"
      },
      "source": [
        "###**Building a CNN based model in Keras:**\n",
        "\n",
        "In this project implementation we have used Convolutional Neural Networks (CNN or CovnNet), a complex feed forward neural networks for image classification with high accuracy. The CNN follows a hierarchical model which works on building a network, like a funnel which finally gives out a fully-connected layer where all the neurons are connected to each other and the output is processed. \n",
        "<br>\n",
        "<br>\n",
        "![CNN](https://www.researchgate.net/profile/Md-Mahin/publication/332407214/figure/fig2/AS:747438860156930@1555214719430/Proposed-adopted-Convolutional-Neural-Network-CNN-model.png)\n",
        "<br>\n",
        "<br>\n",
        "We have added all the layers in a Sequential model. As we are classifying images we used `Conv2D` Layers stacked upon eachother.\n",
        "\n",
        "**Conv2D** is a 2D Convolution Layer which creates a convolution kernel that is  with layers input which helps produce a tensor of outputs. In image processing kernel is a convolution matrix or masks which can be used for blurring, sharpening, embossing, edge detection, and more by doing a convolution between a kernel and an image. In this we use the appropriate number of filters which are to be obtained from the image. It is always in powers of 2. \n",
        "\n",
        "**MaxPooling2D** layers are used to reduce the dimensions of the feature maps as it reduces the number of parameters to learn and the amount of computation performed in the network. The pooling layer summarises the features present in a region of the feature map generated by a convolution layer\n",
        "\n",
        "**Dropout** layers are added to prevent a model from overfitting and increasing the time and space complexity. This happens due to the co-adoptation of individual neurons in NN. Dropout works by randomly setting the outgoing edges of hidden units (neurons that make up hidden layers) to 0 at each update of the training phase. The dropout rate is between 0 and 1.\n",
        "\n",
        "**Flatten** layer converts the pooled feature map to a single column that is passed to the fully connected layer. For example, if flatten is applied to layer having input shape as (batch_size, 2,2), then the output shape of the layer will be (batch_size, 4).\n",
        "\n",
        "**Dense** layer connects to each neuron in the previous layer and recieves inputs from all the neurons. Dense layers adds an interesting non-linearity property by modelling any mathematical function. The dense layer is found to be the most commonly used layer in the models.\n",
        "\n",
        "The input layer of the model is a `Cov2D` layer with `32 filters`, kernel size of `(3, 3)`, input shape of `(224, 224, 3)` and activation function `'relu'`.\n",
        "\n",
        "Then a stack of `Conv2D` layer, `MaxPooling2D` Layer and a `Dropout` layers are created for feature extraction. \n",
        "\n",
        "We have used `sigmoid` activation function in the output layer as we have to predict whether it is a COVID or Non-COVID X-Ray which is a binary classification problem. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXjR3BZ49SOQ"
      },
      "source": [
        "# Build CNN Based Model in Keras\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size = (3,3), activation = 'relu', input_shape = (224, 224, 3)))\n",
        "  \n",
        "model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation = 'relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation = 'sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3LtoW8l0D0E"
      },
      "source": [
        "###**Compiling the Model:**\n",
        "\n",
        "After successfully creating the model, we have to compile the model with three parameters like `loss`, `optimizer`, and `metrics`. \n",
        "\n",
        "- We use `binary_crossentropy` as we are working on binary classification problem. \n",
        "- Among the optimizers like `adam`, `adagrad`, `sgd`, and `rmsprop`, we have selected the `adam` optimizer with its default learning rate (0.001). \n",
        "- We have taken `accuracy` from metrics. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTeN5T1sAYl5"
      },
      "source": [
        "# Compile the model\n",
        "model.compile(loss = \"binary_crossentropy\", \n",
        "              optimizer = 'adam', \n",
        "              metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IYwxxe5BUAm",
        "outputId": "7278c805-8652-45da-f5c5-bf6315955747"
      },
      "source": [
        "# Summary of the Model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 222, 222, 32)      896       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 220, 220, 64)      18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 110, 110, 64)      0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 110, 110, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 108, 108, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 54, 54, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 54, 54, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 52, 52, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 26, 26, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 26, 26, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 86528)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                5537856   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 5,668,097\n",
            "Trainable params: 5,668,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dkStD6V2v-p"
      },
      "source": [
        "###**Processing the Training and Testing Images:**\n",
        "\n",
        "As we are having less number of images we have to apply some Data Augmentation techniques for making our training process more effecient and time saving too. For this we can use `ImageDataGenerator` which can rescale, flip, shrink and apply many more transformations on our images to make the network learn better. \n",
        "\n",
        "Then we create train generator and validation generator with batch size of 32 images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdfDcdUUBYHT"
      },
      "source": [
        "# Processing the images\n",
        "train_datagen = image.ImageDataGenerator(\n",
        "    rescale = 1/255.,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip = True\n",
        ")\n",
        "\n",
        "val_datagen = image.ImageDataGenerator(rescale = 1/255.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHv_FIOCCaw8",
        "outputId": "b14681d5-6d9b-41be-941e-424c12a5ac88"
      },
      "source": [
        "batch_size = 32\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'Dataset/Train',\n",
        "    target_size = (224, 224),\n",
        "    batch_size = batch_size,\n",
        "    class_mode = 'binary'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 294 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxsZQ3jZDDCD",
        "outputId": "c7cbee79-8a88-4025-8978-dcdc6b068df6"
      },
      "source": [
        "val_generator = val_datagen.flow_from_directory(\n",
        "    'Dataset/Val',\n",
        "    target_size = (224, 224),\n",
        "    batch_size = batch_size,\n",
        "    class_mode = 'binary'  \n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 98 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz31VCQcyGsT"
      },
      "source": [
        "### **Add Checkpoints to the model:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nH31gDQyySVp"
      },
      "source": [
        "\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
        "\n",
        "filepath = \"saved_models/weights-imporoved-epoch-{epoch:02d}-val_acc-{val_accuracy:.2f}.hdf5\"\n",
        "\n",
        "'''\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath, \n",
        "    monitor = 'val_accuracy',\n",
        "    verbose = 1,\n",
        "    save_best_only = True,\n",
        "    mode = 'max')\n",
        "\n",
        "callbacks_list = [checkpoint]\n",
        "'''\n",
        "\n",
        "class EarlyStoppingByAccVal(Callback):\n",
        "    def __init__(self, monitor='val_accuracy', value=1.0, verbose=1):\n",
        "        super(Callback, self).__init__()\n",
        "        self.monitor = monitor\n",
        "        self.value = value\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
        "\n",
        "        if current > self.value:\n",
        "            if self.verbose > 0:\n",
        "                print(\"Epoch %05d: early stopping THR\" % epoch)\n",
        "            self.model.stop_training = True\n",
        "            filename = f\"cnn-cxr-val_acc-{self.value:.2f}_bs-{batch_size:02d}_epochs-{epoch:02d}.h5\"\n",
        "            model.save(filename)\n",
        "            print(f\"Model saved with {self.value:.2f} % accuracy.\")\n",
        "\n",
        "callbacks_list = [\n",
        "    EarlyStoppingByAccVal(monitor='val_accuracy', value=0.999999, verbose=1),\n",
        "    # EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n",
        "    ModelCheckpoint(filepath, monitor='val_accuracy', save_best_only=True, verbose=1),\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE0-wGXa4fE-"
      },
      "source": [
        "###**Training the Model:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEe2he3rET_D",
        "outputId": "f74b4845-bc01-4dec-d197-14869377915c"
      },
      "source": [
        "epochs = 80\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = 1,\n",
        "    epochs = epochs,\n",
        "    validation_data = val_generator,\n",
        "    validation_steps = 2,\n",
        "    callbacks = callbacks_list\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/80\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1916 - accuracy: 0.9062 - val_loss: 0.1200 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.96875, saving model to saved_models/weights-imporoved-epoch-01-val_acc-0.97.hdf5\n",
            "Epoch 2/80\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.3438 - accuracy: 0.8438 - val_loss: 0.2262 - val_accuracy: 0.9375\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.96875\n",
            "Epoch 3/80\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1693 - accuracy: 0.9062 - val_loss: 0.1463 - val_accuracy: 0.9531\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.96875\n",
            "Epoch 4/80\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.6479 - accuracy: 0.8750 - val_loss: 0.1431 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.96875\n",
            "Epoch 5/80\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0716 - accuracy: 0.9688 - val_loss: 0.1603 - val_accuracy: 0.9375\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.96875\n",
            "Epoch 6/80\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0582 - accuracy: 0.9688 - val_loss: 0.1218 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.96875\n",
            "Epoch 7/80\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2365 - accuracy: 0.9375 - val_loss: 0.1565 - val_accuracy: 0.9375\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.96875\n",
            "Epoch 8/80\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1332 - accuracy: 0.9688 - val_loss: 0.1886 - val_accuracy: 0.9375\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.96875\n",
            "Epoch 9/80\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.8095 - accuracy: 0.7500 - val_loss: 0.1289 - val_accuracy: 0.9688\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.96875\n",
            "Epoch 10/80\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2213 - accuracy: 0.9375 - val_loss: 0.1946 - val_accuracy: 0.9531\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.96875\n",
            "Epoch 11/80\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1630 - accuracy: 0.9688 - val_loss: 0.1400 - val_accuracy: 1.0000\n",
            "Epoch 00010: early stopping THR\n",
            "Model saved with 1.00 % accuracy.\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.96875 to 1.00000, saving model to saved_models/weights-imporoved-epoch-11-val_acc-1.00.hdf5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0sGveO4tJo4"
      },
      "source": [
        "###**Evaluating the Model\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qQmazMZGD0R",
        "outputId": "2d1b3579-0d20-4fa4-9ffb-2c8c37b9b339"
      },
      "source": [
        "# Evaluate the model\n",
        "model.evaluate(val_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 3s 594ms/step - loss: 0.1806 - accuracy: 0.9694\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.18057198822498322, 0.9693877696990967]"
            ]
          },
          "execution_count": 19,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s1oxwNagAiy"
      },
      "source": [
        "train_acc = round(history.history['accuracy'][-1], 2) * 100\n",
        "val_acc = round(history.history['val_accuracy'][-1], 2) * 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7N2qaEKh6-b",
        "outputId": "3008e1fd-3308-495a-949e-48e53af46e2a"
      },
      "source": [
        "print(f\"The Training accuracy is {train_acc}%\")\n",
        "print(f\"The validation accuracy is {val_acc}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Training accuracy is 97.0%\n",
            "The validation accuracy is 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPfHrQlLtQhV"
      },
      "source": [
        "###**Saving the Model:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c47J0DebGRQH",
        "outputId": "4eeb0bba-8b51-4a1c-95e1-43cc9f46e525"
      },
      "source": [
        "filename = \"cnn-cxr-val_acc-{val_acc:.2f}_bs-{batch_size:02d}_epochs-{epoch:02d}.h5\"\n",
        "model.save(filename)\n",
        "print(\"Model saved with {val_acc:.2f} % accuracy.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved with 52.0 % accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdGR_-xVJ1wG"
      },
      "source": [
        "###**Create the Deployment file:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "bFEw7FT-J_QV",
        "outputId": "d21e5ae3-89e4-42d5-b1c7-dd2649367152"
      },
      "source": [
        "# Pickle the file\n",
        "import pickle\n",
        "# Open a file where we want to store the data\n",
        "file = open(f'COVID-Detection_acc-{val_acc}.pkl', 'wb')\n",
        "# Dump the info into the file\n",
        "pickle.dump(model, file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-4645098e9667>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'COVID-Detection_acc-{val_acc}.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Dump the info into the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: can't pickle weakref objects"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuL2vWxXmgbs"
      },
      "source": [
        "###**Creating a Classification Method:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkD3I8f2nsan",
        "outputId": "5b6f3231-4210-4014-8af2-0e1872036fd1"
      },
      "source": [
        "def classify_cxr(img_path):\n",
        "  # Dimensions of input image\n",
        "  img_width, img_height = 224, 224\n",
        "\n",
        "  # Load the saved model\n",
        "  model = load_model(\"cnn-cxr-val_acc-1.00_bs-32_epochs-10.h5\")\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  # Loading the image and reshaping it\n",
        "  img = image.load_img(img_path, target_size = (img_width, img_height))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  images = np.vstack([x])\n",
        "  # Predicting on X-Ray images\n",
        "  predict_class = model.predict(images)\n",
        "  # Returns the predicted class either 1 or 0\n",
        "  return predict_class[0][0]\n",
        "\n",
        "# Selecting images from our Dataset\n",
        "img_base_path = \"Dataset/Train/Normal/\"\n",
        "img_names = os.listdir(img_base_path)\n",
        "print(f\"Total Images in {img_base_path} are {len(img_names)}\")\n",
        "for i in range(len(img_names)):\n",
        "  img_path = img_base_path + img_names[i]\n",
        "\n",
        "  # Obtaining the classification results\n",
        "  classified = classify_cxr(img_path)\n",
        "\n",
        "  # Printing out the Resuls\n",
        "  if (int(classified) == 0):\n",
        "    print(f\"Reslut: COVID Chest XRay\")\n",
        "  else:\n",
        "    print(f\"Reslut: Non-COVID Chest XRay\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Images in Dataset/Train/Normal/ are 147\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: COVID Chest XRay\n",
            "Reslut: COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n",
            "Reslut: Non-COVID Chest XRay\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm1mYp-UucT3"
      },
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}